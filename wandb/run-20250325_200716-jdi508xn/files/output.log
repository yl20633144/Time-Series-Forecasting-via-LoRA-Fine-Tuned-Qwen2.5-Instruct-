Example Preprocessed Sequences: Train Sequence 1: 2.91,0.28;1.79,0.29;1.09,0.27;0.72,0.23;0.54,0.18;0.44,0.14;0.40,0.11;0.40,0.09;0.41,0.07;0.45,0.05;0.50,0.04;0.58,0.03;0.68,0.03;0.81,0.02;0.98,0.02;1.18,0.02;1.44,0.01;1.76,0.01;2.14,0.01;2.60,0.01;3.15,0.02;3.77,0.02;4.45,0.03;5.14,0.04;5.72,0.06;5.97,0.10;5.61,0.16;4.47,0.24;2.95,0.30;1.72,0.31;1.01,0.28;0.65,0.23;0.47,0.19;0.39,0.14;0.35,0.11;0.34,0.09;0.36,0.07;0.39,0.05;0.44,0.04;0.51,0.03;0.60,0.02;0.72,0.02;0.87,0.02;1.05,0.01;1.29,0.01;1.58,0.01;1.94,0.01;2.37,0.01;2.89,0.01;3.50,0.01;4.20,0.02;4.95,0.03;5.68,0.04;6.22,0.07;6.30,0.12;5.55,0.20;4.00,0.28;2.35,0.33;1.28,0.32;0.74,0.27;0.49,0.22;0.37,0.17;0.32,0.13;0.29,0.10;0.30,0.08;0.32,0.06;0.35,0.04;0.40,0.03;0.47,0.03;0.56,0.02;0.68,0.02;0.82,0.01;1.01,0.01;1.24,0.01;1.53,0.01;1.88,0.01;2.32,0.01;2.84,0.01;3.46,0.01;4.18,0.01;4.97,0.02;5.79,0.03;6.48,0.06;6.77,0.10;6.22,0.18;4.64,0.28;2.72,0.35;1.40,0.35;0.76,0.30;0.47,0.24;0.34,0.19;0.27,0.14;0.25,0.11;0.25,0.08;0.26,0.06;0.29,0.05;0.33,0.04;0.38,0.03;0.45,0.02;0.55,0.02
Example Tokenized Sequence: Train Sequence 1: [17, 13, 24, 16, 11, 15, 13, 17, 23, 26, 16, 13, 22, 24, 11, 15, 13, 17, 24, 26, 16, 13, 15, 24, 11, 15, 13, 17, 22, 26, 15, 13, 22, 17, 11, 15, 13, 17, 18, 26, 15, 13, 20, 19, 11, 15, 13, 16, 23, 26, 15, 13, 19, 19, 11, 15, 13, 16, 19, 26, 15, 13, 19, 15, 11, 15, 13, 16, 16, 26, 15, 13, 19, 15, 11, 15, 13, 15, 24, 26, 15, 13, 19, 16, 11, 15, 13, 15, 22, 26, 15, 13, 19, 20, 11, 15, 13, 15, 20, 26, 15, 13, 20, 15, 11, 15, 13, 15, 19, 26, 15, 13, 20, 23, 11, 15, 13, 15, 18, 26, 15, 13, 21, 23, 11, 15, 13, 15, 18, 26, 15, 13, 23, 16, 11, 15, 13, 15, 17, 26, 15, 13, 24, 23, 11, 15, 13, 15, 17, 26, 16, 13, 16, 23, 11, 15, 13, 15, 17, 26, 16, 13, 19, 19, 11, 15, 13, 15, 16, 26, 16, 13, 22, 21, 11, 15, 13, 15, 16, 26, 17, 13, 16, 19, 11, 15, 13, 15, 16, 26, 17, 13, 21, 15, 11, 15, 13, 15, 16, 26, 18, 13, 16, 20, 11, 15, 13, 15, 17, 26, 18, 13, 22, 22, 11, 15, 13, 15, 17, 26, 19, 13, 19, 20, 11, 15, 13, 15, 18, 26, 20, 13, 16, 19, 11, 15, 13, 15, 19, 26, 20, 13, 22, 17, 11, 15, 13, 15, 21, 26, 20, 13, 24, 22, 11, 15, 13, 16, 15, 26, 20, 13, 21, 16, 11, 15, 13, 16, 21, 26, 19, 13, 19, 22, 11, 15, 13, 17, 19, 26, 17, 13, 24, 20, 11, 15, 13, 18, 15, 26, 16, 13, 22, 17, 11, 15, 13, 18, 16, 26, 16, 13, 15, 16, 11, 15, 13, 17, 23, 26, 15, 13, 21, 20, 11, 15, 13, 17, 18, 26, 15, 13, 19, 22, 11, 15, 13, 16, 24, 26, 15, 13, 18, 24, 11, 15, 13, 16, 19, 26, 15, 13, 18, 20, 11, 15, 13, 16, 16, 26, 15, 13, 18, 19, 11, 15, 13, 15, 24, 26, 15, 13, 18, 21, 11, 15, 13, 15, 22, 26, 15, 13, 18, 24, 11, 15, 13, 15, 20, 26, 15, 13, 19, 19, 11, 15, 13, 15, 19, 26, 15, 13, 20, 16, 11, 15, 13, 15, 18, 26, 15, 13, 21, 15, 11, 15, 13, 15, 17, 26, 15, 13, 22, 17, 11, 15, 13, 15, 17, 26, 15, 13, 23, 22, 11, 15, 13, 15, 17, 26, 16, 13, 15, 20, 11, 15, 13, 15, 16, 26, 16, 13, 17, 24, 11, 15, 13, 15, 16, 26, 16, 13, 20, 23, 11, 15, 13, 15, 16, 26, 16, 13, 24, 19, 11, 15, 13, 15, 16, 26, 17, 13, 18, 22, 11, 15, 13, 15, 16, 26, 17, 13, 23, 24, 11, 15, 13, 15, 16, 26, 18, 13, 20, 15, 11, 15, 13, 15, 16, 26, 19, 13, 17, 15, 11, 15, 13, 15, 17, 26, 19, 13, 24, 20, 11, 15, 13, 15, 18, 26, 20, 13, 21, 23, 11, 15, 13, 15, 19, 26, 21, 13, 17, 17, 11, 15, 13, 15, 22, 26, 21, 13, 18, 15, 11, 15, 13, 16, 17, 26, 20, 13, 20, 20, 11, 15, 13, 17, 15, 26, 19, 13, 15, 15, 11, 15, 13, 17, 23, 26, 17, 13, 18, 20, 11, 15, 13, 18, 18, 26, 16, 13, 17, 23, 11, 15, 13, 18, 17, 26, 15, 13, 22, 19, 11, 15, 13, 17, 22, 26, 15, 13, 19, 24, 11, 15, 13, 17, 17, 26, 15, 13, 18, 22, 11, 15, 13, 16, 22, 26, 15, 13, 18, 17, 11, 15, 13, 16, 18, 26, 15, 13, 17, 24, 11, 15, 13, 16, 15, 26, 15, 13, 18, 15, 11, 15, 13, 15, 23, 26, 15, 13, 18, 17, 11, 15, 13, 15, 21, 26, 15, 13, 18, 20, 11, 15, 13, 15, 19, 26, 15, 13, 19, 15, 11, 15, 13, 15, 18, 26, 15, 13, 19, 22, 11, 15, 13, 15, 18, 26, 15, 13, 20, 21, 11, 15, 13, 15, 17, 26, 15, 13, 21, 23, 11, 15, 13, 15, 17, 26, 15, 13, 23, 17, 11, 15, 13, 15, 16, 26, 16, 13, 15, 16, 11, 15, 13, 15, 16, 26, 16, 13, 17, 19, 11, 15, 13, 15, 16, 26, 16, 13, 20, 18, 11, 15, 13, 15, 16, 26, 16, 13, 23, 23, 11, 15, 13, 15, 16, 26, 17, 13, 18, 17, 11, 15, 13, 15, 16, 26, 17, 13, 23, 19, 11, 15, 13, 15, 16, 26, 18, 13, 19, 21, 11, 15, 13, 15, 16, 26, 19, 13, 16, 23, 11, 15, 13, 15, 16, 26, 19, 13, 24, 22, 11, 15, 13, 15, 17, 26, 20, 13, 22, 24, 11, 15, 13, 15, 18, 26, 21, 13, 19, 23, 11, 15, 13, 15, 21, 26, 21, 13, 22, 22, 11, 15, 13, 16, 15, 26, 21, 13, 17, 17, 11, 15, 13, 16, 23, 26, 19, 13, 21, 19, 11, 15, 13, 17, 23, 26, 17, 13, 22, 17, 11, 15, 13, 18, 20, 26, 16, 13, 19, 15, 11, 15, 13, 18, 20, 26, 15, 13, 22, 21, 11, 15, 13, 18, 15, 26, 15, 13, 19, 22, 11, 15, 13, 17, 19, 26, 15, 13, 18, 19, 11, 15, 13, 16, 24, 26, 15, 13, 17, 22, 11, 15, 13, 16, 19, 26, 15, 13, 17, 20, 11, 15, 13, 16, 16, 26, 15, 13, 17, 20, 11, 15, 13, 15, 23, 26, 15, 13, 17, 21, 11, 15, 13, 15, 21, 26, 15, 13, 17, 24, 11, 15, 13, 15, 20, 26, 15, 13, 18, 18, 11, 15, 13, 15, 19, 26, 15, 13, 18, 23, 11, 15, 13, 15, 18, 26, 15, 13, 19, 20, 11, 15, 13, 15, 17, 26, 15, 13, 20, 20, 11, 15, 13, 15, 17]
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 0 that is less than the current step 1. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 3 that is less than the current step 4. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 4 that is less than the current step 5. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[34m[1mwandb[0m: Currently logged in as: [33mliuyihao649[0m ([33mliuyihao649-university-of-cambridge[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin
