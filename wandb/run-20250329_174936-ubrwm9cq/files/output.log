/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 2 that is less than the current step 4. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 3 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 4 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 5 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 6 that is less than the current step 12. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 7 that is less than the current step 14. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Validation Loss: 1.5688
Validation MSE (forecast): 0.4736
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 8 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 9 that is less than the current step 18. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 10 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Validation Loss: 1.4635 Validation MSE (forecast): 0.0648
=== Inference FLOPS Calculation ===
Number of inference steps: 500
Batch size: 8
Sequence length: 512
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 15.68259714362721
=== Inference FLOPS Calculation ===
Number of inference steps: 1000
Batch size: 4
Sequence length: 512
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 15.68259714362721
=== Inference FLOPS Calculation ===
Number of inference steps: 1000
Batch size: 4
Sequence length: 128
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 15.061014220510852
=== Inference FLOPS Calculation ===
Number of inference steps: 1000
Batch size: 4
Sequence length: 768
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 15.87123256601962
Example Preprocessed Sequences: Train Sequence 1: 0.92,0.74;0.56,0.77;0.34,0.70;0.23,0.59;0.17,0.48;0.14,0.37;0.13,0.29;0.12,0.23;0.13,0.17;0.14,0.14;0.16,0.11;0.18,0.08;0.21,0.07;0.26,0.06;0.31,0.05;0.37,0.04;0.45,0.04;0.55,0.03;0.67,0.03;0.82,0.04;0.99,0.04;1.19,0.05;1.40,0.07;1.62,0.10;1.80,0.15;1.88,0.25;1.77,0.41;1.41,0.62;0.93,0.79;0.54,0.82;0.32,0.74;0.20,0.61;0.15,0.49;0.12,0.38;0.11,0.29;0.11,0.22;0.11,0.17;0.12,0.13;0.14,0.10;0.16,0.08;0.19,0.06;0.23,0.05;0.27,0.04;0.33,0.04;0.41,0.03;0.50,0.03;0.61,0.03;0.75,0.03;0.91,0.03;1.10,0.04;1.32,0.05;1.56,0.07;1.79,0.11;1.96,0.18;1.98,0.31;1.75,0.51;1.26,0.75;0.74,0.88;0.40,0.84;0.23,0.72;0.15,0.58;0.12,0.45;0.10,0.34;0.09,0.26;0.09,0.20;0.10,0.15;0.11,0.12;0.13,0.09;0.15,0.07;0.18,0.06;0.21,0.04;0.26,0.04;0.32,0.03;0.39,0.03;0.48,0.02;0.59,0.02;0.73,0.02;0.89,0.03;1.09,0.03;1.32,0.04;1.57,0.06;1.82,0.09;2.04,0.15;2.13,0.26;1.96,0.46;1.46,0.73;0.86,0.92;0.44,0.92;0.24,0.79;0.15,0.63;0.11,0.49;0.09,0.37;0.08,0.28;0.08,0.21;0.08,0.16;0.09,0.12;0.10,0.09;0.12,0.07;0.14,0.06;0.17,0.04
shape of train text: 999
Example Tokenized Sequence: Train Sequence 1: [15, 13, 24, 17, 11, 15, 13, 22, 19, 26, 15, 13, 20, 21, 11, 15, 13, 22, 22, 26, 15, 13, 18, 19, 11, 15, 13, 22, 15, 26, 15, 13, 17, 18, 11, 15, 13, 20, 24, 26, 15, 13, 16, 22, 11, 15, 13, 19, 23, 26, 15, 13, 16, 19, 11, 15, 13, 18, 22, 26, 15, 13, 16, 18, 11, 15, 13, 17, 24, 26, 15, 13, 16, 17, 11, 15, 13, 17, 18, 26, 15, 13, 16, 18, 11, 15, 13, 16, 22, 26, 15, 13, 16, 19, 11, 15, 13, 16, 19, 26, 15, 13, 16, 21, 11, 15, 13, 16, 16, 26, 15, 13, 16, 23, 11, 15, 13, 15, 23, 26, 15, 13, 17, 16, 11, 15, 13, 15, 22, 26, 15, 13, 17, 21, 11, 15, 13, 15, 21, 26, 15, 13, 18, 16, 11, 15, 13, 15, 20, 26, 15, 13, 18, 22, 11, 15, 13, 15, 19, 26, 15, 13, 19, 20, 11, 15, 13, 15, 19, 26, 15, 13, 20, 20, 11, 15, 13, 15, 18, 26, 15, 13, 21, 22, 11, 15, 13, 15, 18, 26, 15, 13, 23, 17, 11, 15, 13, 15, 19, 26, 15, 13, 24, 24, 11, 15, 13, 15, 19, 26, 16, 13, 16, 24, 11, 15, 13, 15, 20, 26, 16, 13, 19, 15, 11, 15, 13, 15, 22, 26, 16, 13, 21, 17, 11, 15, 13, 16, 15, 26, 16, 13, 23, 15, 11, 15, 13, 16, 20, 26, 16, 13, 23, 23, 11, 15, 13, 17, 20, 26, 16, 13, 22, 22, 11, 15, 13, 19, 16, 26, 16, 13, 19, 16, 11, 15, 13, 21, 17, 26, 15, 13, 24, 18, 11, 15, 13, 22, 24, 26, 15, 13, 20, 19, 11, 15, 13, 23, 17, 26, 15, 13, 18, 17, 11, 15, 13, 22, 19, 26, 15, 13, 17, 15, 11, 15, 13, 21, 16, 26, 15, 13, 16, 20, 11, 15, 13, 19, 24, 26, 15, 13, 16, 17, 11, 15, 13, 18, 23, 26, 15, 13, 16, 16, 11, 15, 13, 17, 24, 26, 15, 13, 16, 16, 11, 15, 13, 17, 17, 26, 15, 13, 16, 16, 11, 15, 13, 16, 22, 26, 15, 13, 16, 17, 11, 15, 13, 16, 18, 26, 15, 13, 16, 19, 11, 15, 13, 16, 15, 26, 15, 13, 16, 21, 11, 15, 13, 15, 23, 26, 15, 13, 16, 24, 11, 15, 13, 15, 21, 26, 15, 13, 17, 18, 11, 15, 13, 15, 20, 26, 15, 13, 17, 22, 11, 15, 13, 15, 19, 26, 15, 13, 18, 18, 11, 15, 13, 15, 19, 26, 15, 13, 19, 16, 11, 15, 13, 15, 18, 26, 15, 13, 20, 15, 11, 15, 13, 15, 18, 26, 15, 13, 21, 16, 11, 15, 13, 15, 18, 26, 15, 13, 22, 20, 11, 15, 13, 15, 18, 26, 15, 13, 24, 16, 11, 15, 13, 15, 18, 26, 16, 13, 16, 15, 11, 15, 13, 15, 19, 26, 16, 13, 18, 17, 11, 15, 13, 15, 20, 26, 16, 13, 20, 21, 11, 15, 13, 15, 22, 26, 16, 13, 22, 24, 11, 15, 13, 16, 16, 26, 16, 13, 24, 21, 11, 15, 13, 16, 23, 26, 16, 13, 24, 23, 11, 15, 13, 18, 16, 26, 16, 13, 22, 20, 11, 15, 13, 20, 16, 26, 16, 13, 17, 21, 11, 15, 13, 22, 20, 26, 15, 13, 22, 19, 11, 15, 13, 23, 23, 26, 15, 13, 19, 15, 11, 15, 13, 23, 19, 26, 15, 13, 17, 18, 11, 15, 13, 22, 17, 26, 15, 13, 16, 20, 11, 15, 13, 20, 23, 26, 15, 13, 16, 17, 11, 15, 13, 19, 20, 26, 15, 13, 16, 15, 11, 15, 13, 18, 19, 26, 15, 13, 15, 24, 11, 15, 13, 17, 21, 26, 15, 13, 15, 24, 11, 15, 13, 17, 15, 26, 15, 13, 16, 15, 11, 15, 13, 16, 20, 26, 15, 13, 16, 16, 11, 15, 13, 16, 17, 26, 15, 13, 16, 18, 11, 15, 13, 15, 24, 26, 15, 13, 16, 20, 11, 15, 13, 15, 22, 26, 15, 13, 16, 23, 11, 15, 13, 15, 21, 26, 15, 13, 17, 16, 11, 15, 13, 15, 19, 26, 15, 13, 17, 21, 11, 15, 13, 15, 19, 26, 15, 13, 18, 17, 11, 15, 13, 15, 18, 26, 15, 13, 18, 24, 11, 15, 13, 15, 18, 26, 15, 13, 19, 23, 11, 15, 13, 15, 17, 26, 15, 13, 20, 24, 11, 15, 13, 15, 17, 26, 15, 13, 22, 18, 11, 15, 13, 15, 17, 26, 15, 13, 23, 24, 11, 15, 13, 15, 18, 26, 16, 13, 15, 24, 11, 15, 13, 15, 18, 26, 16, 13, 18, 17, 11, 15, 13, 15, 19, 26, 16, 13, 20, 22, 11, 15, 13, 15, 21, 26, 16, 13, 23, 17, 11, 15, 13, 15, 24, 26, 17, 13, 15, 19, 11, 15, 13, 16, 20, 26, 17, 13, 16, 18, 11, 15, 13, 17, 21, 26, 16, 13, 24, 21, 11, 15, 13, 19, 21, 26, 16, 13, 19, 21, 11, 15, 13, 22, 18, 26, 15, 13, 23, 21, 11, 15, 13, 24, 17, 26, 15, 13, 19, 19, 11, 15, 13, 24, 17, 26, 15, 13, 17, 19, 11, 15, 13, 22, 24, 26, 15, 13, 16, 20, 11, 15, 13, 21, 18, 26, 15, 13, 16, 16, 11, 15, 13, 19, 24, 26, 15, 13, 15, 24, 11, 15, 13, 18, 22, 26, 15, 13, 15, 23, 11, 15, 13, 17, 23, 26, 15, 13, 15, 23, 11, 15, 13, 17, 16, 26, 15, 13, 15, 23, 11, 15, 13, 16, 21, 26, 15, 13, 15, 24, 11, 15, 13, 16, 17, 26, 15, 13, 16, 15, 11, 15, 13, 15, 24, 26, 15, 13, 16, 17, 11, 15, 13, 15, 22, 26, 15, 13, 16, 19, 11, 15, 13, 15, 21, 26, 15, 13, 16, 22, 11, 15, 13, 15, 19]
shape of tokenized train text 999
Example Preprocessed Sequences: Train Sequence 1: 0.92,0.74;0.56,0.77;0.34,0.70;0.23,0.59;
shape of train text: 999
Example Tokenized Sequence: Train Sequence 1: [15, 13, 24, 17, 11, 15, 13, 22, 19, 26, 15, 13, 20, 21, 11, 15, 13, 22, 22, 26, 15, 13, 18, 19, 11, 15, 13, 22, 15, 26, 15, 13, 17, 18, 11, 15, 13, 20, 24, 26]
shape of tokenized train text 999
=== Inference FLOPS Calculation ===
Number of inference steps: 1000
Batch size: 4
Sequence length: 768
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 16
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 15.870987716330351
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 768
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 16
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.870987716330351
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 768
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.870769220352356
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 512
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.682120212067874
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 128
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.060515347554395
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 128
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.060415504159732
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 128
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.06071496550873
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 712
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.835358406073043
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 512
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.682311047546753
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 128
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.06071496550873
=== Inference FLOPS Calculation ===
Number of inference steps: 1
Batch size: 4
Sequence length: 768
Hidden dimension: 896
Transformer blocks: 24
Attention heads: 14
FFN ratio: 4.0
------------------------------------
Total FLOPS for inference: 12.8709546179443
