{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Preprocesses and tokenized the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor import load_and_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the preprocesser to preprocess the dataset, and tokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"lotka_volterra_data.h5\"\n",
    "\n",
    "# Use the function to load and preprocess the data\n",
    "train_texts, val_texts, test_texts = load_and_preprocess(\n",
    "    file_path,\n",
    "    decimal_places=2,\n",
    "    max_target_value=9.99\n",
    ")\n",
    "\n",
    "# Demonstrate tokenization using Qwen2.5\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenized_train=[]\n",
    "tokenized_val=[]\n",
    "tokenized_test=[]\n",
    "for i in range(len(train_texts)):\n",
    "    tokenized_train.append(tokenizer(train_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "for i in range(len(val_texts)): \n",
    "    tokenized_val.append(tokenizer(val_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])    \n",
    "for i in range(len(test_texts)):\n",
    "    tokenized_test.append(tokenizer(test_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show examples of preprocessed data and tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Preprocessed Sequences: Train Sequence 1: 0.92,0.74;0.56,0.77;0.34,0.70;0.23,0.59;\n",
      "shape of train text: 999\n",
      "Example Tokenized Sequence: Train Sequence 1: [15, 13, 24, 17, 11, 15, 13, 22, 19, 26, 15, 13, 20, 21, 11, 15, 13, 22, 22, 26, 15, 13, 18, 19, 11, 15, 13, 22, 15, 26, 15, 13, 17, 18, 11, 15, 13, 20, 24, 26]\n",
      "shape of tokenized train text 999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example Preprocessed Sequences: Train Sequence {1}:\", train_texts[1][:40])\n",
    "print(f\"shape of train text:\", len(train_texts[1]))\n",
    "print(f\"Example Tokenized Sequence: Train Sequence {1}:\",tokenized_train[1].tolist()[:40])\n",
    "print(f\"shape of tokenized train text\", len(tokenized_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Evaluate the untrained Qwen2.5-Instruct model’s forecasting ability on this tokenized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.qwen import load_qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "original_model, tokenizer = load_qwen()\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliuyihao649\u001b[0m (\u001b[33mliuyihao649-university-of-cambridge\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250405_031736-o32q33ht</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/o32q33ht' target=\"_blank\">untrained-noLora_evaluation</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/o32q33ht' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/o32q33ht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Evaluation on Untrained Qwen2.5-Instruct ===\n",
      "Average Cross-Entropy Loss: 1.7850\n",
      "Average MSE (Forecast):  0.2702\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"Qwen_baseline\", name=\"untrained-noLora_evaluation\",reinit=True) \n",
    "avg_loss, avg_mse = evaluation(\n",
    "    original_model, tokenizer, tokenized_val, context_ratio=0.7\n",
    ")\n",
    "\n",
    "\n",
    "print(\"=== Baseline Evaluation on Untrained Qwen2.5-Instruct ===\")\n",
    "print(f\"Average Cross-Entropy Loss: {avg_loss:.4f}\")\n",
    "print(f\"Average MSE (Forecast):  {avg_mse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Map each operation to its flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment FLOPS Calculation ===\n",
      "Number of optimizer steps: 500\n",
      "Batch size: 4\n",
      "Sequence length: 512\n",
      "Hidden dimension: 896\n",
      "Transformer blocks: 24\n",
      "Attention heads: 14\n",
      "FFN ratio: 4.0\n",
      "------------------------------------\n",
      "Total FLOPS for this experiment: 15.381090216403893\n"
     ]
    }
   ],
   "source": [
    "from src.flops import flops_for_experiment\n",
    "import numpy as np\n",
    "\n",
    "# Suppose we want to run inference for 10 steps (e.g., 10 forward passes)\n",
    "num_steps = 500\n",
    "\n",
    "# Hypothetical model configuration\n",
    "batch_size = 4    # number of samples per batch\n",
    "seq_len = 512  # input sequence length\n",
    "hidden_dim = 896   # model hidden dimension\n",
    "num_layers = 24    # number of Transformer blocks\n",
    "num_heads = 14     # number of attention heads\n",
    "r=4                 # Lora rank\n",
    "ffn_ratio = 4.0     # typical ratio for feed-forward layer size\n",
    "\n",
    "# Compute total FLOPS for the inference experiment\n",
    "total_inference_flops = flops_for_experiment(\n",
    "    num_steps=num_steps,\n",
    "    batch_size=batch_size,\n",
    "    seq_len=seq_len,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ffn_ratio=ffn_ratio,\n",
    "    r=r,\n",
    "    training=True\n",
    ")\n",
    "\n",
    "print(\"=== Experiment FLOPS Calculation ===\")\n",
    "print(f\"Number of optimizer steps: {num_steps}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Hidden dimension: {hidden_dim}\")\n",
    "print(f\"Transformer blocks: {num_layers}\")\n",
    "print(f\"Attention heads: {num_heads}\")\n",
    "print(f\"FFN ratio: {ffn_ratio}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Total FLOPS for this experiment: {np.log10(total_inference_flops)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1214.7513152658278"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**17 / (10**12.682 *0.7 +10**12.682 *10 + 10**12.061 +10**12.871 *4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3(a): Adapt the lora implementation, and train the 0.5B parameters Qwen model with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lora_skeleton import apply_lora, load_data, train_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids, test_ids = load_data(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): LoRALinear(\n",
       "            (original_linear): Linear(in_features=896, out_features=896, bias=True)\n",
       "          )\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): LoRALinear(\n",
       "            (original_linear): Linear(in_features=896, out_features=128, bias=True)\n",
       "          )\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_rank4_model,_ = load_qwen()\n",
    "apply_lora(lr1_rank4_model, r=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>▁</td></tr><tr><td>avg_mse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>1.785</td></tr><tr><td>avg_mse</td><td>0.27025</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">untrained-noLora_evaluation</strong> at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/o32q33ht' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/o32q33ht</a><br> View project at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline</a><br>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250405_031736-o32q33ht/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250405_032607-q9abzwx0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/q9abzwx0' target=\"_blank\">default_value_train_model</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/q9abzwx0' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/q9abzwx0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "Training:   0%|          | 0/500 [00:00<?, ?it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training:  10%|█         | 50/500 [01:07<09:03,  1.21s/it, loss=0.766]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: loss = 0.7663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 100/500 [02:07<07:36,  1.14s/it, loss=0.795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: loss = 0.7953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 150/500 [03:06<06:38,  1.14s/it, loss=0.989]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150: loss = 0.9891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 200/500 [04:06<06:31,  1.30s/it, loss=0.694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: loss = 0.6940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 250/500 [05:06<04:55,  1.18s/it, loss=0.612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250: loss = 0.6120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 300/500 [06:05<04:06,  1.23s/it, loss=0.568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: loss = 0.5684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 350/500 [07:05<03:02,  1.21s/it, loss=0.798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350: loss = 0.7984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 400/500 [08:04<01:58,  1.18s/it, loss=0.623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: loss = 0.6226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 450/500 [09:03<01:01,  1.24s/it, loss=0.542]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: loss = 0.5419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [10:01<00:00,  1.20s/it, loss=0.685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: loss = 0.6846\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen_3a_train_model\", name=\"default_value_train_model\", reinit=True)\n",
    "losses_default_train=train_lora(lr1_rank4_model, train_ids, max_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutate the model on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250405_035408-whw68umh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/whw68umh' target=\"_blank\">default_value_evaluation_model</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/whw68umh' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/whw68umh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6563 Validation MSE (forecast): 0.1248\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen_3a_train_model\", name=\"default_value_evaluation_model\", reinit=True)\n",
    "val_loss_1_4, val_mse_1_4 = evaluation(lr1_rank4_model, tokenizer, tokenized_val,context_ratio=0.7)\n",
    "print(f\"Validation Loss: {val_loss_1_4:.4f} \"\n",
    "      f\"Validation MSE (forecast): {val_mse_1_4:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the untrained QWen model, apply LoRA on the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>▁</td></tr><tr><td>avg_mse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>1.65628</td></tr><tr><td>avg_mse</td><td>0.1248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">default_value_evaluation_model</strong> at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/whw68umh' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/whw68umh</a><br> View project at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a><br>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250405_035408-whw68umh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250405_035525-q9f70z21</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/q9f70z21' target=\"_blank\">LoRA_untrained_evaluation</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/q9f70z21' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/q9f70z21</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7850\n",
      "Validation MSE (forecast): 0.2702\n"
     ]
    }
   ],
   "source": [
    "untrained_model, _ = load_qwen()\n",
    "apply_lora(untrained_model, r=4)\n",
    "untrained_model.to(device)\n",
    "wandb.init(project=\"Qwen_3a_train_model\", name=\"LoRA_untrained_evaluation\", reinit=True)\n",
    "\n",
    "val_lora_untrained_loss, val_lora_untrained_mse = evaluation(untrained_model, tokenizer, tokenized_val, context_ratio=0.7)\n",
    "print(f\"Validation Loss: {val_lora_untrained_loss:.4f}\")\n",
    "print(f\"Validation MSE (forecast): {val_lora_untrained_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(b): Make hyperparameter tuning and using metrics to select a best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a new method for evaluate the validation dataset by using dataloader. Previously we predicted many tokens for each of the 10 sequences, so we can check the performance for untrained/trained and LoRA/non-LoRA models. In this step, we do not explictly compare the performance. Instead, we use metrics, so we use the whole validation dataset but only predict 20-30 tokens for each of all the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm \n",
    "from accelerate import Accelerator   \n",
    "def evaluate_lora_mse(model, tokenizer, val_ids, context_ratio=0.5, batch_size=4, max_gen_tokens=30):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on validation data using MSE.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Qwen2.5 model (with LoRA applied).\n",
    "        tokenizer: The tokenizer used for encoding/decoding.\n",
    "        val_ids: A tensor of shape [num_sequences, seq_len].\n",
    "        context_ratio: The percentage of tokens to use as context (e.g. 0.7).\n",
    "        batch_size: Batch size for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        avg_mse: Average Mean Squared Error over all sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_dataset = TensorDataset(val_ids)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    model, val_loader = accelerator.prepare(model, val_loader)\n",
    "\n",
    "\n",
    "    mses = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        # batch: (batch_size, seq_len)\n",
    "            input_seqs = batch[0]\n",
    "\n",
    "            for seq in input_seqs:\n",
    "                context_ids, target_ids = split_context_target(seq, context_ratio)\n",
    "                target_ids = target_ids[:max_gen_tokens]\n",
    "\n",
    "                # 1. Cross-Entropy Loss over context+target\n",
    "                full_seq = seq.unsqueeze(0)\n",
    "                output = model(full_seq, labels=full_seq)\n",
    "                losses.append(output.loss.item())\n",
    "\n",
    "                # 2. Generation & MSE calculation\n",
    "                input_ids = context_ids.unsqueeze(0)\n",
    "                max_new_tokens = len(target_ids)\n",
    "                generated = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "\n",
    "                generated_ids = generated[0][len(context_ids):]\n",
    "                pred_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "                true_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "\n",
    "                pred_numbers = decode_tokens_to_numbers(pred_text)\n",
    "                true_numbers = decode_tokens_to_numbers(true_text)\n",
    "\n",
    "                if len(pred_numbers) == len(true_numbers) and len(pred_numbers) > 0:\n",
    "                    mse = np.mean((np.array(pred_numbers) - np.array(true_numbers)) ** 2)\n",
    "                    mses.append(mse)\n",
    "\n",
    "    avg_ce_loss = np.mean(losses) if len(losses) > 0 else float(\"inf\")\n",
    "    avg_mse = np.mean(mses) if len(mses) > 0 else float(\"inf\")\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"  Average Cross-Entropy Loss: {avg_ce_loss:.4f}\")\n",
    "    print(f\"  Average MSE (forecast):     {avg_mse:.4f}\")\n",
    "\n",
    "    return avg_ce_loss, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, tokenizer, tokenized_data, context_ratio: float = 0.7):\n",
    "    \"\"\"\n",
    "    Evaluates the model in a fully autoregressive manner using model.generate.\n",
    "    \n",
    "    For each sequence:\n",
    "      1. Split the sequence into context and target using split_context_target.\n",
    "      2. Use model.generate (with output_scores=True and return_dict_in_generate=True)\n",
    "         to generate all target tokens at once.\n",
    "      3. Extract the per-token logits (scores) for each generated token and compute the \n",
    "         cross-entropy loss against the ground truth token.\n",
    "      4. Compute the average loss over the generated tokens and log the loss curve.\n",
    "      5. Decode the generated tokens and the ground truth target tokens into numeric values,\n",
    "         and compute the Mean Squared Error (MSE) for forecast evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: The Qwen2.5-Instruct model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        tokenized_data (List[torch.Tensor]): List of 1D token ID tensors.\n",
    "        context_ratio (float): Fraction of tokens used as context.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float]: The average cross-entropy loss and MSE over evaluated sequences.\n",
    "    \"\"\"\n",
    "   \n",
    "    num_eval = 10  # Evaluate first 10 sequences\n",
    "    all_seq_losses = []\n",
    "    all_seq_mses = []\n",
    "    \n",
    "    for i in range(num_eval):\n",
    "        # Retrieve sequence and split into context and target\n",
    "        seq = tokenized_data[i].to(device)\n",
    "        context_ids, target_ids = split_context_target(seq, context_ratio)\n",
    "        target_ids = target_ids[:100]\n",
    "        input_ids = context_ids.unsqueeze(0)  # Shape: (1, context_length)\n",
    "        \n",
    "        # Generate tokens autoregressively using model.generate with scores output\n",
    "        with torch.no_grad():\n",
    "            gen_output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=len(target_ids),\n",
    "                do_sample=False,  # Greedy decoding\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        \n",
    "        # gen_output.sequences contains context + generated tokens.\n",
    "        generated_ids = gen_output.sequences[0]\n",
    "        # gen_output.scores is a tuple of logits for each generated token, each with shape (batch_size, vocab_size)\n",
    "        scores = gen_output.scores\n",
    "        \n",
    "        # Compute per-token loss using the returned scores and corresponding ground truth token\n",
    "        token_losses = []\n",
    "        for j, score in enumerate(scores):\n",
    "            # Ground truth token for step j is target_ids[j]\n",
    "            gt_token = target_ids[j].unsqueeze(0)  # Shape: (1,)\n",
    "            loss_j = torch.nn.functional.cross_entropy(score, gt_token)\n",
    "            token_losses.append(loss_j.item())\n",
    "        avg_loss_seq = np.mean(token_losses)\n",
    "        all_seq_losses.append(avg_loss_seq)\n",
    "        \n",
    "        # For forecast evaluation, compare generated tokens (excluding context) to ground truth target tokens\n",
    "        generated_target_ids = generated_ids[len(context_ids):]\n",
    "        pred_text = tokenizer.decode(generated_target_ids, skip_special_tokens=True)\n",
    "        true_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "        pred_numbers = decode_tokens_to_numbers(pred_text)\n",
    "        true_numbers = decode_tokens_to_numbers(true_text)\n",
    "        mse = float(\"inf\")\n",
    "        if len(pred_numbers) == len(true_numbers) and len(pred_numbers) > 0:\n",
    "            mse = np.mean((np.array(pred_numbers) - np.array(true_numbers)) ** 2)\n",
    "        all_seq_mses.append(mse)\n",
    "        \n",
    "        # plot and log the loss curve\n",
    "       \n",
    "        plt.figure()\n",
    "        plt.plot(token_losses, label=\"Token Loss\")\n",
    "        plt.title(f\"Loss Curve for Sequence {i}\")\n",
    "        plt.xlabel(\"Prediction Step\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.legend()\n",
    "        wandb.log({f\"loss_curve_seq_{i}\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "        \n",
    "        # plot and log forecast comparison (requires numeric reshaping, e.g., into (-1, 2))\n",
    "        try:\n",
    "            pred_array = np.array(pred_numbers).reshape(-1, 2)\n",
    "            true_array = np.array(true_numbers).reshape(-1, 2)\n",
    "            plt.figure()\n",
    "            plt.plot(true_array[:, 0], label=\"True Prey\")\n",
    "            plt.plot(true_array[:, 1], label=\"True Predator\", linestyle=\"--\")\n",
    "            plt.plot(pred_array[:, 0], label=\"Predicted Prey\")\n",
    "            plt.plot(pred_array[:, 1], label=\"Predicted Predator\", linestyle=\"--\")\n",
    "            plt.title(f\"Forecast Comparison for Sequence {i}\", fontsize=15)\n",
    "            plt.xlabel(\"Time Step\", fontsize=15)\n",
    "            plt.ylabel(\"Population\", fontsize=15)   \n",
    "            plt.legend(fontsize=12)\n",
    "            wandb.log({f\"forecast_seq_{i}\": wandb.Image(plt)})\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Sequence {i}: Error in plotting forecast: {e}\")\n",
    "        \n",
    "    \n",
    "    avg_loss_overall = np.mean(all_seq_losses) if all_seq_losses else float(\"inf\")\n",
    "    avg_mse_overall = np.mean(all_seq_mses) if all_seq_mses else float(\"inf\")\n",
    "    wandb.log({\n",
    "        \"avg_loss\": avg_loss_overall,\n",
    "        \"avg_mse\": avg_mse_overall,\n",
    "    })\n",
    "    return avg_loss_overall, avg_mse_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>▁</td></tr><tr><td>avg_mse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>1.785</td></tr><tr><td>avg_mse</td><td>0.27025</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LoRA_untrained_evaluation</strong> at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/q9f70z21' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/q9f70z21</a><br> View project at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a><br>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250405_035525-q9f70z21/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250405_234617-1xscjh51</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/1xscjh51' target=\"_blank\">default_value_evaluation_model</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/1xscjh51' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/1xscjh51</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6563 Validation MSE (forecast): 0.1248\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen_3a_train_model\", name=\"default_value_evaluation_model\", reinit=True)\n",
    "val_loss_1_4_new, val_mse_1_4_new = evaluation(lr1_rank4_model, tokenizer, tokenized_val,context_ratio=0.7)\n",
    "print(f\"Validation Loss: {val_loss_1_4_new:.4f} \"\n",
    "      f\"Validation MSE (forecast): {val_mse_1_4_new:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
