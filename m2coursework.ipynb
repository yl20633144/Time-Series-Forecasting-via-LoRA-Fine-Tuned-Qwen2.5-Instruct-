{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Preprocesses and tokenized the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import load_and_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the preprocesser to preprocess the dataset, and tokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"lotka_volterra_data.h5\"\n",
    "\n",
    "# Use the function to load and preprocess the data\n",
    "train_texts, val_texts, test_texts = load_and_preprocess(\n",
    "    file_path,\n",
    "    decimal_places=2,\n",
    "    max_target_value=9.99\n",
    ")\n",
    "\n",
    "# Demonstrate tokenization using Qwen2.5\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenized_train=[]\n",
    "tokenized_val=[]\n",
    "tokenized_test=[]\n",
    "for i in range(len(train_texts)):\n",
    "    tokenized_train.append(tokenizer(train_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "for i in range(len(val_texts)): \n",
    "    tokenized_val.append(tokenizer(val_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])    \n",
    "for i in range(len(test_texts)):\n",
    "    tokenized_test.append(tokenizer(test_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show examples of preprocessed data and tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Preprocessed Sequences: Train Sequence 1: 0.92,0.74;0.56,0.77;0.34,0.70;0.23,0.59;0.17,0.48;0.14,0.37;0.13,0.29;0.12,0.23;0.13,0.17;0.14,0.14;0.16,0.11;0.18,0.08;0.21,0.07;0.26,0.06;0.31,0.05;0.37,0.04;0.45,0.04;0.55,0.03;0.67,0.03;0.82,0.04;0.99,0.04;1.19,0.05;1.40,0.07;1.62,0.10;1.80,0.15;1.88,0.25;1.77,0.41;1.41,0.62;0.93,0.79;0.54,0.82;0.32,0.74;0.20,0.61;0.15,0.49;0.12,0.38;0.11,0.29;0.11,0.22;0.11,0.17;0.12,0.13;0.14,0.10;0.16,0.08;0.19,0.06;0.23,0.05;0.27,0.04;0.33,0.04;0.41,0.03;0.50,0.03;0.61,0.03;0.75,0.03;0.91,0.03;1.10,0.04;1.32,0.05;1.56,0.07;1.79,0.11;1.96,0.18;1.98,0.31;1.75,0.51;1.26,0.75;0.74,0.88;0.40,0.84;0.23,0.72;0.15,0.58;0.12,0.45;0.10,0.34;0.09,0.26;0.09,0.20;0.10,0.15;0.11,0.12;0.13,0.09;0.15,0.07;0.18,0.06;0.21,0.04;0.26,0.04;0.32,0.03;0.39,0.03;0.48,0.02;0.59,0.02;0.73,0.02;0.89,0.03;1.09,0.03;1.32,0.04;1.57,0.06;1.82,0.09;2.04,0.15;2.13,0.26;1.96,0.46;1.46,0.73;0.86,0.92;0.44,0.92;0.24,0.79;0.15,0.63;0.11,0.49;0.09,0.37;0.08,0.28;0.08,0.21;0.08,0.16;0.09,0.12;0.10,0.09;0.12,0.07;0.14,0.06;0.17,0.04\n",
      "shape of train text: 999\n",
      "Example Tokenized Sequence: Train Sequence 1: [15, 13, 24, 17, 11, 15, 13, 22, 19, 26, 15, 13, 20, 21, 11, 15, 13, 22, 22, 26, 15, 13, 18, 19, 11, 15, 13, 22, 15, 26, 15, 13, 17, 18, 11, 15, 13, 20, 24, 26, 15, 13, 16, 22, 11, 15, 13, 19, 23, 26, 15, 13, 16, 19, 11, 15, 13, 18, 22, 26, 15, 13, 16, 18, 11, 15, 13, 17, 24, 26, 15, 13, 16, 17, 11, 15, 13, 17, 18, 26, 15, 13, 16, 18, 11, 15, 13, 16, 22, 26, 15, 13, 16, 19, 11, 15, 13, 16, 19, 26, 15, 13, 16, 21, 11, 15, 13, 16, 16, 26, 15, 13, 16, 23, 11, 15, 13, 15, 23, 26, 15, 13, 17, 16, 11, 15, 13, 15, 22, 26, 15, 13, 17, 21, 11, 15, 13, 15, 21, 26, 15, 13, 18, 16, 11, 15, 13, 15, 20, 26, 15, 13, 18, 22, 11, 15, 13, 15, 19, 26, 15, 13, 19, 20, 11, 15, 13, 15, 19, 26, 15, 13, 20, 20, 11, 15, 13, 15, 18, 26, 15, 13, 21, 22, 11, 15, 13, 15, 18, 26, 15, 13, 23, 17, 11, 15, 13, 15, 19, 26, 15, 13, 24, 24, 11, 15, 13, 15, 19, 26, 16, 13, 16, 24, 11, 15, 13, 15, 20, 26, 16, 13, 19, 15, 11, 15, 13, 15, 22, 26, 16, 13, 21, 17, 11, 15, 13, 16, 15, 26, 16, 13, 23, 15, 11, 15, 13, 16, 20, 26, 16, 13, 23, 23, 11, 15, 13, 17, 20, 26, 16, 13, 22, 22, 11, 15, 13, 19, 16, 26, 16, 13, 19, 16, 11, 15, 13, 21, 17, 26, 15, 13, 24, 18, 11, 15, 13, 22, 24, 26, 15, 13, 20, 19, 11, 15, 13, 23, 17, 26, 15, 13, 18, 17, 11, 15, 13, 22, 19, 26, 15, 13, 17, 15, 11, 15, 13, 21, 16, 26, 15, 13, 16, 20, 11, 15, 13, 19, 24, 26, 15, 13, 16, 17, 11, 15, 13, 18, 23, 26, 15, 13, 16, 16, 11, 15, 13, 17, 24, 26, 15, 13, 16, 16, 11, 15, 13, 17, 17, 26, 15, 13, 16, 16, 11, 15, 13, 16, 22, 26, 15, 13, 16, 17, 11, 15, 13, 16, 18, 26, 15, 13, 16, 19, 11, 15, 13, 16, 15, 26, 15, 13, 16, 21, 11, 15, 13, 15, 23, 26, 15, 13, 16, 24, 11, 15, 13, 15, 21, 26, 15, 13, 17, 18, 11, 15, 13, 15, 20, 26, 15, 13, 17, 22, 11, 15, 13, 15, 19, 26, 15, 13, 18, 18, 11, 15, 13, 15, 19, 26, 15, 13, 19, 16, 11, 15, 13, 15, 18, 26, 15, 13, 20, 15, 11, 15, 13, 15, 18, 26, 15, 13, 21, 16, 11, 15, 13, 15, 18, 26, 15, 13, 22, 20, 11, 15, 13, 15, 18, 26, 15, 13, 24, 16, 11, 15, 13, 15, 18, 26, 16, 13, 16, 15, 11, 15, 13, 15, 19, 26, 16, 13, 18, 17, 11, 15, 13, 15, 20, 26, 16, 13, 20, 21, 11, 15, 13, 15, 22, 26, 16, 13, 22, 24, 11, 15, 13, 16, 16, 26, 16, 13, 24, 21, 11, 15, 13, 16, 23, 26, 16, 13, 24, 23, 11, 15, 13, 18, 16, 26, 16, 13, 22, 20, 11, 15, 13, 20, 16, 26, 16, 13, 17, 21, 11, 15, 13, 22, 20, 26, 15, 13, 22, 19, 11, 15, 13, 23, 23, 26, 15, 13, 19, 15, 11, 15, 13, 23, 19, 26, 15, 13, 17, 18, 11, 15, 13, 22, 17, 26, 15, 13, 16, 20, 11, 15, 13, 20, 23, 26, 15, 13, 16, 17, 11, 15, 13, 19, 20, 26, 15, 13, 16, 15, 11, 15, 13, 18, 19, 26, 15, 13, 15, 24, 11, 15, 13, 17, 21, 26, 15, 13, 15, 24, 11, 15, 13, 17, 15, 26, 15, 13, 16, 15, 11, 15, 13, 16, 20, 26, 15, 13, 16, 16, 11, 15, 13, 16, 17, 26, 15, 13, 16, 18, 11, 15, 13, 15, 24, 26, 15, 13, 16, 20, 11, 15, 13, 15, 22, 26, 15, 13, 16, 23, 11, 15, 13, 15, 21, 26, 15, 13, 17, 16, 11, 15, 13, 15, 19, 26, 15, 13, 17, 21, 11, 15, 13, 15, 19, 26, 15, 13, 18, 17, 11, 15, 13, 15, 18, 26, 15, 13, 18, 24, 11, 15, 13, 15, 18, 26, 15, 13, 19, 23, 11, 15, 13, 15, 17, 26, 15, 13, 20, 24, 11, 15, 13, 15, 17, 26, 15, 13, 22, 18, 11, 15, 13, 15, 17, 26, 15, 13, 23, 24, 11, 15, 13, 15, 18, 26, 16, 13, 15, 24, 11, 15, 13, 15, 18, 26, 16, 13, 18, 17, 11, 15, 13, 15, 19, 26, 16, 13, 20, 22, 11, 15, 13, 15, 21, 26, 16, 13, 23, 17, 11, 15, 13, 15, 24, 26, 17, 13, 15, 19, 11, 15, 13, 16, 20, 26, 17, 13, 16, 18, 11, 15, 13, 17, 21, 26, 16, 13, 24, 21, 11, 15, 13, 19, 21, 26, 16, 13, 19, 21, 11, 15, 13, 22, 18, 26, 15, 13, 23, 21, 11, 15, 13, 24, 17, 26, 15, 13, 19, 19, 11, 15, 13, 24, 17, 26, 15, 13, 17, 19, 11, 15, 13, 22, 24, 26, 15, 13, 16, 20, 11, 15, 13, 21, 18, 26, 15, 13, 16, 16, 11, 15, 13, 19, 24, 26, 15, 13, 15, 24, 11, 15, 13, 18, 22, 26, 15, 13, 15, 23, 11, 15, 13, 17, 23, 26, 15, 13, 15, 23, 11, 15, 13, 17, 16, 26, 15, 13, 15, 23, 11, 15, 13, 16, 21, 26, 15, 13, 15, 24, 11, 15, 13, 16, 17, 26, 15, 13, 16, 15, 11, 15, 13, 15, 24, 26, 15, 13, 16, 17, 11, 15, 13, 15, 22, 26, 15, 13, 16, 19, 11, 15, 13, 15, 21, 26, 15, 13, 16, 22, 11, 15, 13, 15, 19]\n",
      "shape of tokenized train text 999\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example Preprocessed Sequences: Train Sequence {1}:\", train_texts[1])\n",
    "print(f\"shape of train text:\", len(train_texts[10]))\n",
    "print(f\"Example Tokenized Sequence: Train Sequence {1}:\",tokenized_train[1].tolist())\n",
    "print(f\"shape of tokenized train text\", len(tokenized_train[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Evaluate the untrained Qwen2.5-Instruct model’s forecasting ability on this tokenized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen import load_qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "original_model, tokenizer = load_qwen()\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_context_target(token_ids: torch.Tensor, context_ratio: float = 0.8):\n",
    "    \"\"\"\n",
    "    Splits a 1D tensor of token IDs into context and target parts.\n",
    "    \n",
    "    Args:\n",
    "        token_ids (torch.Tensor): A 1D tensor of token IDs.\n",
    "        context_ratio (float): Fraction of tokens to use as context.\n",
    "    \n",
    "    Returns:\n",
    "        (context_ids, target_ids) (torch.Tensor, torch.Tensor)\n",
    "    \"\"\"\n",
    "    total_length = len(token_ids)\n",
    "    context_length = int(total_length * context_ratio)\n",
    "    context_ids = token_ids[:context_length]\n",
    "    target_ids = token_ids[context_length:]\n",
    "    return context_ids, target_ids\n",
    "\n",
    "def decode_tokens_to_numbers(text: str):\n",
    "    \"\"\"\n",
    "    Decodes a LLMTIME-formatted string into a list of numeric values.\n",
    "    Example format: \"0.25,1.50;0.27,1.47;0.31,1.42\"\n",
    "    \n",
    "    We split by semicolon to separate timesteps, then by comma for variables,\n",
    "    and parse each as a float.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The decoded text from the model's output.\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: A flat list of numeric values (prey, predator, prey, predator, ...).\n",
    "    \"\"\"\n",
    "    numbers = []\n",
    "    timesteps = text.split(\";\")\n",
    "    for step in timesteps:\n",
    "        # Split each timestep by commas\n",
    "        parts = step.split(\",\")\n",
    "        for p in parts:\n",
    "            try:\n",
    "                # Convert the string to float if possible\n",
    "                val = float(p.strip())\n",
    "                numbers.append(val)\n",
    "            except ValueError:\n",
    "                # If conversion fails (e.g., empty string), skip\n",
    "                continue\n",
    "    return numbers\n",
    "\n",
    "###############################################################################\n",
    "# Main Evaluation Function\n",
    "###############################################################################\n",
    "\n",
    "def evaluation(model, tokenizer, tokenized_data, context_ratio: float = 0.7):\n",
    "    \"\"\"\n",
    "    Evaluates the model in a fully autoregressive manner using model.generate.\n",
    "    \n",
    "    For each sequence:\n",
    "      1. Split the sequence into context and target using split_context_target.\n",
    "      2. Use model.generate (with output_scores=True and return_dict_in_generate=True)\n",
    "         to generate all target tokens at once.\n",
    "      3. Extract the per-token logits (scores) for each generated token and compute the \n",
    "         cross-entropy loss against the ground truth token.\n",
    "      4. Compute the average loss over the generated tokens and log the loss curve.\n",
    "      5. Decode the generated tokens and the ground truth target tokens into numeric values,\n",
    "         and compute the Mean Squared Error (MSE) for forecast evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: The Qwen2.5-Instruct model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        tokenized_data (List[torch.Tensor]): List of 1D token ID tensors.\n",
    "        context_ratio (float): Fraction of tokens used as context.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float]: The average cross-entropy loss and MSE over evaluated sequences.\n",
    "    \"\"\"\n",
    "   \n",
    "    num_eval = 10  # Evaluate first 10 sequences\n",
    "    all_seq_losses = []\n",
    "    all_seq_mses = []\n",
    "    \n",
    "    for i in range(num_eval):\n",
    "        # Retrieve sequence and split into context and target\n",
    "        seq = tokenized_data[i].to(device)\n",
    "        context_ids, target_ids = split_context_target(seq, context_ratio)\n",
    "        target_ids = target_ids[:100]\n",
    "        input_ids = context_ids.unsqueeze(0)  # Shape: (1, context_length)\n",
    "        \n",
    "        # Generate tokens autoregressively using model.generate with scores output\n",
    "        with torch.no_grad():\n",
    "            gen_output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=len(target_ids),\n",
    "                do_sample=False,  # Greedy decoding\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        \n",
    "        # gen_output.sequences contains context + generated tokens.\n",
    "        generated_ids = gen_output.sequences[0]\n",
    "        # gen_output.scores is a tuple of logits for each generated token, each with shape (batch_size, vocab_size)\n",
    "        scores = gen_output.scores\n",
    "        \n",
    "        # Compute per-token loss using the returned scores and corresponding ground truth token\n",
    "        token_losses = []\n",
    "        for j, score in enumerate(scores):\n",
    "            # Ground truth token for step j is target_ids[j]\n",
    "            gt_token = target_ids[j].unsqueeze(0)  # Shape: (1,)\n",
    "            loss_j = torch.nn.functional.cross_entropy(score, gt_token)\n",
    "            token_losses.append(loss_j.item())\n",
    "        avg_loss_seq = np.mean(token_losses)\n",
    "        all_seq_losses.append(avg_loss_seq)\n",
    "        \n",
    "        # For forecast evaluation, compare generated tokens (excluding context) to ground truth target tokens\n",
    "        generated_target_ids = generated_ids[len(context_ids):]\n",
    "        pred_text = tokenizer.decode(generated_target_ids, skip_special_tokens=True)\n",
    "        true_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "        pred_numbers = decode_tokens_to_numbers(pred_text)\n",
    "        true_numbers = decode_tokens_to_numbers(true_text)\n",
    "        mse = float(\"inf\")\n",
    "        if len(pred_numbers) == len(true_numbers) and len(pred_numbers) > 0:\n",
    "            mse = np.mean((np.array(pred_numbers) - np.array(true_numbers)) ** 2)\n",
    "        all_seq_mses.append(mse)\n",
    "        \n",
    "        # plot and log the loss curve\n",
    "       \n",
    "        plt.figure()\n",
    "        plt.plot(token_losses, label=\"Token Loss\")\n",
    "        plt.title(f\"Loss Curve for Sequence {i}\")\n",
    "        plt.xlabel(\"Prediction Step\")\n",
    "        plt.ylabel(\"Cross-Entropy Loss\")\n",
    "        plt.legend()\n",
    "        wandb.log({f\"loss_curve_seq_{i}\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "        \n",
    "        # plot and log forecast comparison (requires numeric reshaping, e.g., into (-1, 2))\n",
    "        try:\n",
    "            pred_array = np.array(pred_numbers).reshape(-1, 2)\n",
    "            true_array = np.array(true_numbers).reshape(-1, 2)\n",
    "            plt.figure()\n",
    "            plt.plot(true_array[:, 0], label=\"True Prey\")\n",
    "            plt.plot(true_array[:, 1], label=\"True Predator\", linestyle=\"--\")\n",
    "            plt.plot(pred_array[:, 0], label=\"Predicted Prey\")\n",
    "            plt.plot(pred_array[:, 1], label=\"Predicted Predator\", linestyle=\"--\")\n",
    "            plt.title(f\"Forecast Comparison for Sequence {i}\")\n",
    "            plt.legend()\n",
    "            wandb.log({f\"forecast_seq_{i}\": wandb.Image(plt)})\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Sequence {i}: Error in plotting forecast: {e}\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"loss_per_sequence\": avg_loss_seq,\n",
    "            \"mse_per_sequence\": mse,\n",
    "        }, step=i+1)\n",
    "    \n",
    "    avg_loss_overall = np.mean(all_seq_losses) if all_seq_losses else float(\"inf\")\n",
    "    avg_mse_overall = np.mean(all_seq_mses) if all_seq_mses else float(\"inf\")\n",
    "    wandb.log({\n",
    "        \"avg_loss\": avg_loss_overall,\n",
    "        \"avg_mse\": avg_mse_overall,\n",
    "    })\n",
    "    return avg_loss_overall, avg_mse_overall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliuyihao649\u001b[0m (\u001b[33mliuyihao649-university-of-cambridge\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250329_173233-uqfa331w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/uqfa331w' target=\"_blank\">untrained-noLora_evaluation</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/uqfa331w' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/uqfa331w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/uqfa331w?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x34f391ee0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"Qwen_baseline\", name=\"untrained-noLora_evaluation\",reinit=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 4. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 12. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7 that is less than the current step 14. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Evaluation on Untrained Qwen2.5-Instruct ===\n",
      "Average Cross-Entropy Loss: 1.6455\n",
      "Average MSE (Forecast):  0.2022\n"
     ]
    }
   ],
   "source": [
    "avg_loss, avg_mse = evaluation(\n",
    "    original_model, tokenizer, tokenized_val, context_ratio=0.7\n",
    ")\n",
    "\n",
    "# 4) Print results\n",
    "print(\"=== Baseline Evaluation on Untrained Qwen2.5-Instruct ===\")\n",
    "print(f\"Average Cross-Entropy Loss: {avg_loss:.4f}\")\n",
    "print(f\"Average MSE (Forecast):  {avg_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Map each operation to its flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference FLOPS Calculation ===\n",
      "Number of inference steps: 1000\n",
      "Batch size: 4\n",
      "Sequence length: 512\n",
      "Hidden dimension: 896\n",
      "Transformer blocks: 24\n",
      "Attention heads: 14\n",
      "FFN ratio: 4.0\n",
      "------------------------------------\n",
      "Total FLOPS for inference: 15.68259714362721\n"
     ]
    }
   ],
   "source": [
    "from flops import flops_for_experiment\n",
    "import numpy as np\n",
    "\n",
    "# Suppose we want to run inference for 10 steps (e.g., 10 forward passes)\n",
    "num_steps = 1000\n",
    "\n",
    "# Hypothetical model configuration\n",
    "batch_size = 4    # number of samples per batch\n",
    "seq_len = 512     # input sequence length\n",
    "hidden_dim = 896   # model hidden dimension\n",
    "num_layers = 24    # number of Transformer blocks\n",
    "num_heads = 14       # number of attention heads\n",
    "r=4                 # Lora rank\n",
    "ffn_ratio = 4.0     # typical ratio for feed-forward layer size\n",
    "\n",
    "# Compute total FLOPS for the inference experiment\n",
    "total_inference_flops = flops_for_experiment(\n",
    "    num_steps=num_steps,\n",
    "    batch_size=batch_size,\n",
    "    seq_len=seq_len,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ffn_ratio=ffn_ratio,\n",
    "    r=r,\n",
    "    training=True\n",
    ")\n",
    "\n",
    "print(\"=== Inference FLOPS Calculation ===\")\n",
    "print(f\"Number of inference steps: {num_steps}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Hidden dimension: {hidden_dim}\")\n",
    "print(f\"Transformer blocks: {num_layers}\")\n",
    "print(f\"Attention heads: {num_heads}\")\n",
    "print(f\"FFN ratio: {ffn_ratio}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Total FLOPS for inference: {np.log10(total_inference_flops)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1353.5962801256442"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**17 / (10**12.68 *9 + 10**12.06 +10**12.87 *4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3(a): Adapt the lora implementation, and train the 0.5B parameters Qwen model with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_skeleton import apply_lora, load_data, train_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, val_ids, test_ids = load_data(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): LoRALinear(\n",
       "            (original_linear): Linear(in_features=896, out_features=896, bias=True)\n",
       "          )\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): LoRALinear(\n",
       "            (original_linear): Linear(in_features=896, out_features=128, bias=True)\n",
       "          )\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_rank4_model,_ = load_qwen()\n",
    "apply_lora(lr1_rank4_model, r=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>▁</td></tr><tr><td>avg_mse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>1.64547</td></tr><tr><td>avg_mse</td><td>0.20217</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">untrained-noLora_evaluation</strong> at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/uqfa331w' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline/runs/uqfa331w</a><br> View project at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_baseline</a><br>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250329_173233-uqfa331w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tried to log to step 9 that is less than the current step 18. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tried to log to step 10 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250329_173311-4avig8c1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/4avig8c1' target=\"_blank\">default_value_train_model</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/4avig8c1' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/4avig8c1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/4avig8c1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x3ad0251c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen_3a_train_model\", name=\"default_value_train_model\", reinit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/700 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training:   7%|▋         | 50/700 [01:06<12:48,  1.18s/it, loss=0.551]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: loss = 0.5513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 100/700 [02:05<11:30,  1.15s/it, loss=0.676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: loss = 0.6756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 150/700 [03:03<10:40,  1.16s/it, loss=0.647]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150: loss = 0.6474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 200/700 [04:05<10:20,  1.24s/it, loss=0.75] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: loss = 0.7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 250/700 [05:07<09:00,  1.20s/it, loss=0.703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250: loss = 0.7033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 300/700 [06:07<08:12,  1.23s/it, loss=0.56] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: loss = 0.5595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 350/700 [07:07<07:04,  1.21s/it, loss=0.512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350: loss = 0.5115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 400/700 [08:06<06:04,  1.22s/it, loss=0.583]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: loss = 0.5833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 450/700 [09:06<05:12,  1.25s/it, loss=0.576]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: loss = 0.5758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████▏  | 500/700 [10:05<04:17,  1.29s/it, loss=0.527]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: loss = 0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|███████▊  | 550/700 [11:04<02:57,  1.18s/it, loss=0.558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 550: loss = 0.5578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 600/700 [12:06<02:01,  1.21s/it, loss=0.492]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600: loss = 0.4921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 650/700 [13:08<01:14,  1.48s/it, loss=0.593]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 650: loss = 0.5931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 700/700 [14:09<00:00,  1.21s/it, loss=0.492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700: loss = 0.4924\n"
     ]
    }
   ],
   "source": [
    "losses_default_train=train_lora(lr1_rank4_model, train_ids, max_steps=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutate the model on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▄██▅▃▅▄▃▃▄▅▃▂▃▄▁▂▄▂▂▂▃▃▂▃▂▂▁▁▃▂▂▄▃▂▃▂▂▃▂</td></tr><tr><td>step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.49242</td></tr><tr><td>step</td><td>699</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">default_value_train_model</strong> at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/4avig8c1' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/4avig8c1</a><br> View project at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250329_173311-4avig8c1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250329_174828-se34pn38</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/se34pn38' target=\"_blank\">default_value_evaluation_model</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/se34pn38' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/se34pn38</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/se34pn38?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x3abc27bf0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen_3a_train_model\", name=\"default_value_evaluation_model\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss_1_4, val_mse_1_4 = evaluate_lora_mse(lr1_rank4_model, tokenizer,val_ids, context_ratio=0.5,max_gen_tokens=20)\n",
    "\n",
    "# print(f\"Validation Loss: {val_loss_1_4:.4f}\")\n",
    "# print(f\"Validation MSE (forecast): {val_mse_1_4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 4. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 12. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7 that is less than the current step 14. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9 that is less than the current step 18. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.4635 Validation MSE (forecast): 0.0648\n"
     ]
    }
   ],
   "source": [
    "val_loss_1_4, val_mse_1_4 = evaluation(lr1_rank4_model, tokenizer, tokenized_val,context_ratio=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.4635 Validation MSE (forecast): 0.0648\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Loss: {val_loss_1_4:.4f} \"\n",
    "      f\"Validation MSE (forecast): {val_mse_1_4:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the untrained QWen model, apply LoRA on the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): LoRALinear(\n",
       "            (original_linear): Linear(in_features=896, out_features=896, bias=True)\n",
       "          )\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): LoRALinear(\n",
       "            (original_linear): Linear(in_features=896, out_features=128, bias=True)\n",
       "          )\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_model, _ = load_qwen()\n",
    "apply_lora(untrained_model, r=4)\n",
    "untrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>▁</td></tr><tr><td>avg_mse</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>1.46346</td></tr><tr><td>avg_mse</td><td>0.06484</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">default_value_evaluation_model</strong> at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/se34pn38' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/se34pn38</a><br> View project at: <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a><br>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250329_174828-se34pn38/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liuyihao/Desktop/m2cw/wandb/run-20250329_174936-ubrwm9cq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/ubrwm9cq' target=\"_blank\">LoRA_untrained_evaluation</a></strong> to <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/ubrwm9cq' target=\"_blank\">https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/ubrwm9cq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/liuyihao649-university-of-cambridge/Qwen_3a_train_model/runs/ubrwm9cq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x33d228740>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen_3a_train_model\", name=\"LoRA_untrained_evaluation\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 4. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 6. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 12. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7 that is less than the current step 14. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9 that is less than the current step 18. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5688\n",
      "Validation MSE (forecast): 0.4736\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_lora_untrained_loss, val_lora_untrained_mse = evaluation(untrained_model, tokenizer, tokenized_val, context_ratio=0.5)\n",
    "print(f\"Validation Loss: {val_lora_untrained_loss:.4f}\")\n",
    "print(f\"Validation MSE (forecast): {val_lora_untrained_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss_untrained, val_mse_untrained = evaluate_lora_mse(untrained_model, tokenizer,val_ids, context_ratio=0.5,max_gen_tokens=20)\n",
    "\n",
    "# print(f\"Validation Loss: {val_loss_1_4:.4f}\")\n",
    "# print(f\"Validation MSE (forecast): {val_mse_1_4:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(b): Make hyperparameter tuning and using metrics to select a best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a new method for evaluate the validation dataset by using dataloader. Previously we predicted many tokens for each of the 10 sequences, so we can check the performance for untrained/trained and LoRA/non-LoRA models. In this step, we do not explictly compare the performance. Instead, we use metrics, so we use the whole validation dataset but only predict 20-30 tokens for each of all the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm \n",
    "from accelerate import Accelerator   \n",
    "def evaluate_lora_mse(model, tokenizer, val_ids, context_ratio=0.5, batch_size=4, max_gen_tokens=30):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on validation data using MSE.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Qwen2.5 model (with LoRA applied).\n",
    "        tokenizer: The tokenizer used for encoding/decoding.\n",
    "        val_ids: A tensor of shape [num_sequences, seq_len].\n",
    "        context_ratio: The percentage of tokens to use as context (e.g. 0.7).\n",
    "        batch_size: Batch size for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        avg_mse: Average Mean Squared Error over all sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_dataset = TensorDataset(val_ids)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    model, val_loader = accelerator.prepare(model, val_loader)\n",
    "\n",
    "\n",
    "    mses = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        # batch: (batch_size, seq_len)\n",
    "            input_seqs = batch[0]\n",
    "\n",
    "            for seq in input_seqs:\n",
    "                context_ids, target_ids = split_context_target(seq, context_ratio)\n",
    "                target_ids = target_ids[:max_gen_tokens]\n",
    "\n",
    "                # 1. Cross-Entropy Loss over context+target\n",
    "                full_seq = seq.unsqueeze(0)\n",
    "                output = model(full_seq, labels=full_seq)\n",
    "                losses.append(output.loss.item())\n",
    "\n",
    "                # 2. Generation & MSE calculation\n",
    "                input_ids = context_ids.unsqueeze(0)\n",
    "                max_new_tokens = len(target_ids)\n",
    "                generated = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "\n",
    "                generated_ids = generated[0][len(context_ids):]\n",
    "                pred_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "                true_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "\n",
    "                pred_numbers = decode_tokens_to_numbers(pred_text)\n",
    "                true_numbers = decode_tokens_to_numbers(true_text)\n",
    "\n",
    "                if len(pred_numbers) == len(true_numbers) and len(pred_numbers) > 0:\n",
    "                    mse = np.mean((np.array(pred_numbers) - np.array(true_numbers)) ** 2)\n",
    "                    mses.append(mse)\n",
    "\n",
    "    avg_ce_loss = np.mean(losses) if len(losses) > 0 else float(\"inf\")\n",
    "    avg_mse = np.mean(mses) if len(mses) > 0 else float(\"inf\")\n",
    "\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"  Average Cross-Entropy Loss: {avg_ce_loss:.4f}\")\n",
    "    print(f\"  Average MSE (forecast):     {avg_mse:.4f}\")\n",
    "\n",
    "    return avg_ce_loss, avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss_base, val_mse_base = evaluate_lora_mse(\n",
    "#    original_model, tokenizer, val_ids, context_ratio=0.5,batch_size=4,max_gen_tokens=20\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"=== Evaluation on Validation Set (Untrained Qwen2.5) ===\")\n",
    "# print(f\"Cross-Entropy Loss:  {val_loss_base:.4f}\")\n",
    "# print(f\"Forecast MSE:        {val_mse_base:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
