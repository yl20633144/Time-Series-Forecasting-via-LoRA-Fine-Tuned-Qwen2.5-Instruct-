{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Preprocesses and tokenized the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import load_and_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the preprocesser to preprocess the dataset, and tokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/lotka_volterra_data.h5\"\n",
    "\n",
    "# Use the function to load and preprocess the data\n",
    "train_texts, val_texts, test_texts = load_and_preprocess(\n",
    "    file_path,\n",
    "    decimal_places=2,\n",
    "    max_target_value=10.0\n",
    ")\n",
    "\n",
    "# Demonstrate tokenization using Qwen2.5\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenized_train=[]\n",
    "tokenized_val=[]\n",
    "tokenized_test=[]\n",
    "for i in range(len(train_texts)):\n",
    "    tokenized_train.append(tokenizer(train_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "for i in range(len(val_texts)): \n",
    "    tokenized_val.append(tokenizer(val_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])    \n",
    "for i in range(len(test_texts)):\n",
    "    tokenized_test.append(tokenizer(test_texts[i], return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show examples of preprocessed data and tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Preprocessed Sequences: Train Sequence 1: 2.91,0.28;1.79,0.29;1.09,0.27;0.72,0.23;0.54,0.18;0.44,0.14;0.4,0.11;0.4,0.09;0.41,0.07;0.45,0.05;0.5,0.04;0.58,0.03;0.68,0.03;0.81,0.02;0.98,0.02;1.18,0.02;1.44,0.01;1.76,0.01;2.14,0.01;2.6,0.01;3.15,0.02;3.77,0.02;4.45,0.03;5.14,0.04;5.72,0.06;5.97,0.1;5.61,0.16;4.47,0.24;2.95,0.3;1.72,0.31;1.01,0.28;0.65,0.23;0.47,0.19;0.39,0.14;0.35,0.11;0.34,0.09;0.36,0.07;0.39,0.05;0.44,0.04;0.51,0.03;0.6,0.02;0.72,0.02;0.87,0.02;1.05,0.01;1.29,0.01;1.58,0.01;1.94,0.01;2.37,0.01;2.89,0.01;3.5,0.01;4.2,0.02;4.95,0.03;5.68,0.04;6.22,0.07;6.3,0.12;5.55,0.2;4.0,0.28;2.35,0.33;1.28,0.32;0.74,0.27;0.49,0.22;0.37,0.17;0.32,0.13;0.29,0.1;0.3,0.08;0.32,0.06;0.35,0.04;0.4,0.03;0.47,0.03;0.56,0.02;0.68,0.02;0.82,0.01;1.01,0.01;1.24,0.01;1.53,0.01;1.88,0.01;2.32,0.01;2.84,0.01;3.46,0.01;4.18,0.01;4.97,0.02;5.79,0.03;6.48,0.06;6.77,0.1;6.22,0.18;4.64,0.28;2.72,0.35;1.4,0.35;0.76,0.3;0.47,0.24;0.34,0.19;0.27,0.14;0.25,0.11;0.25,0.08;0.26,0.06;0.29,0.05;0.33,0.04;0.38,0.03;0.45,0.02;0.55,0.02\n",
      "Example Tokenized Sequence: Train Sequence 1: [17, 13, 24, 16, 11, 15, 13, 17, 23, 26, 16, 13, 22, 24, 11, 15, 13, 17, 24, 26, 16, 13, 15, 24, 11, 15, 13, 17, 22, 26, 15, 13, 22, 17, 11, 15, 13, 17, 18, 26, 15, 13, 20, 19, 11, 15, 13, 16, 23, 26, 15, 13, 19, 19, 11, 15, 13, 16, 19, 26, 15, 13, 19, 11, 15, 13, 16, 16, 26, 15, 13, 19, 11, 15, 13, 15, 24, 26, 15, 13, 19, 16, 11, 15, 13, 15, 22, 26, 15, 13, 19, 20, 11, 15, 13, 15, 20, 26, 15, 13, 20, 11, 15, 13, 15, 19, 26, 15, 13, 20, 23, 11, 15, 13, 15, 18, 26, 15, 13, 21, 23, 11, 15, 13, 15, 18, 26, 15, 13, 23, 16, 11, 15, 13, 15, 17, 26, 15, 13, 24, 23, 11, 15, 13, 15, 17, 26, 16, 13, 16, 23, 11, 15, 13, 15, 17, 26, 16, 13, 19, 19, 11, 15, 13, 15, 16, 26, 16, 13, 22, 21, 11, 15, 13, 15, 16, 26, 17, 13, 16, 19, 11, 15, 13, 15, 16, 26, 17, 13, 21, 11, 15, 13, 15, 16, 26, 18, 13, 16, 20, 11, 15, 13, 15, 17, 26, 18, 13, 22, 22, 11, 15, 13, 15, 17, 26, 19, 13, 19, 20, 11, 15, 13, 15, 18, 26, 20, 13, 16, 19, 11, 15, 13, 15, 19, 26, 20, 13, 22, 17, 11, 15, 13, 15, 21, 26, 20, 13, 24, 22, 11, 15, 13, 16, 26, 20, 13, 21, 16, 11, 15, 13, 16, 21, 26, 19, 13, 19, 22, 11, 15, 13, 17, 19, 26, 17, 13, 24, 20, 11, 15, 13, 18, 26, 16, 13, 22, 17, 11, 15, 13, 18, 16, 26, 16, 13, 15, 16, 11, 15, 13, 17, 23, 26, 15, 13, 21, 20, 11, 15, 13, 17, 18, 26, 15, 13, 19, 22, 11, 15, 13, 16, 24, 26, 15, 13, 18, 24, 11, 15, 13, 16, 19, 26, 15, 13, 18, 20, 11, 15, 13, 16, 16, 26, 15, 13, 18, 19, 11, 15, 13, 15, 24, 26, 15, 13, 18, 21, 11, 15, 13, 15, 22, 26, 15, 13, 18, 24, 11, 15, 13, 15, 20, 26, 15, 13, 19, 19, 11, 15, 13, 15, 19, 26, 15, 13, 20, 16, 11, 15, 13, 15, 18, 26, 15, 13, 21, 11, 15, 13, 15, 17, 26, 15, 13, 22, 17, 11, 15, 13, 15, 17, 26, 15, 13, 23, 22, 11, 15, 13, 15, 17, 26, 16, 13, 15, 20, 11, 15, 13, 15, 16, 26, 16, 13, 17, 24, 11, 15, 13, 15, 16, 26, 16, 13, 20, 23, 11, 15, 13, 15, 16, 26, 16, 13, 24, 19, 11, 15, 13, 15, 16, 26, 17, 13, 18, 22, 11, 15, 13, 15, 16, 26, 17, 13, 23, 24, 11, 15, 13, 15, 16, 26, 18, 13, 20, 11, 15, 13, 15, 16, 26, 19, 13, 17, 11, 15, 13, 15, 17, 26, 19, 13, 24, 20, 11, 15, 13, 15, 18, 26, 20, 13, 21, 23, 11, 15, 13, 15, 19, 26, 21, 13, 17, 17, 11, 15, 13, 15, 22, 26, 21, 13, 18, 11, 15, 13, 16, 17, 26, 20, 13, 20, 20, 11, 15, 13, 17, 26, 19, 13, 15, 11, 15, 13, 17, 23, 26, 17, 13, 18, 20, 11, 15, 13, 18, 18, 26, 16, 13, 17, 23, 11, 15, 13, 18, 17, 26, 15, 13, 22, 19, 11, 15, 13, 17, 22, 26, 15, 13, 19, 24, 11, 15, 13, 17, 17, 26, 15, 13, 18, 22, 11, 15, 13, 16, 22, 26, 15, 13, 18, 17, 11, 15, 13, 16, 18, 26, 15, 13, 17, 24, 11, 15, 13, 16, 26, 15, 13, 18, 11, 15, 13, 15, 23, 26, 15, 13, 18, 17, 11, 15, 13, 15, 21, 26, 15, 13, 18, 20, 11, 15, 13, 15, 19, 26, 15, 13, 19, 11, 15, 13, 15, 18, 26, 15, 13, 19, 22, 11, 15, 13, 15, 18, 26, 15, 13, 20, 21, 11, 15, 13, 15, 17, 26, 15, 13, 21, 23, 11, 15, 13, 15, 17, 26, 15, 13, 23, 17, 11, 15, 13, 15, 16, 26, 16, 13, 15, 16, 11, 15, 13, 15, 16, 26, 16, 13, 17, 19, 11, 15, 13, 15, 16, 26, 16, 13, 20, 18, 11, 15, 13, 15, 16, 26, 16, 13, 23, 23, 11, 15, 13, 15, 16, 26, 17, 13, 18, 17, 11, 15, 13, 15, 16, 26, 17, 13, 23, 19, 11, 15, 13, 15, 16, 26, 18, 13, 19, 21, 11, 15, 13, 15, 16, 26, 19, 13, 16, 23, 11, 15, 13, 15, 16, 26, 19, 13, 24, 22, 11, 15, 13, 15, 17, 26, 20, 13, 22, 24, 11, 15, 13, 15, 18, 26, 21, 13, 19, 23, 11, 15, 13, 15, 21, 26, 21, 13, 22, 22, 11, 15, 13, 16, 26, 21, 13, 17, 17, 11, 15, 13, 16, 23, 26, 19, 13, 21, 19, 11, 15, 13, 17, 23, 26, 17, 13, 22, 17, 11, 15, 13, 18, 20, 26, 16, 13, 19, 11, 15, 13, 18, 20, 26, 15, 13, 22, 21, 11, 15, 13, 18, 26, 15, 13, 19, 22, 11, 15, 13, 17, 19, 26, 15, 13, 18, 19, 11, 15, 13, 16, 24, 26, 15, 13, 17, 22, 11, 15, 13, 16, 19, 26, 15, 13, 17, 20, 11, 15, 13, 16, 16, 26, 15, 13, 17, 20, 11, 15, 13, 15, 23, 26, 15, 13, 17, 21, 11, 15, 13, 15, 21, 26, 15, 13, 17, 24, 11, 15, 13, 15, 20, 26, 15, 13, 18, 18, 11, 15, 13, 15, 19, 26, 15, 13, 18, 23, 11, 15, 13, 15, 18, 26, 15, 13, 19, 20, 11, 15, 13, 15, 17, 26, 15, 13, 20, 20, 11, 15, 13, 15, 17]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example Preprocessed Sequences: Train Sequence {1}:\", train_texts[1])\n",
    "\n",
    "\n",
    "print(f\"Example Tokenized Sequence: Train Sequence {1}:\",tokenized_train[1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Evaluate the untrained Qwen2.5-Instruct modelâ€™s forecasting ability on this tokenized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Qwen2ForCausalLM(\n",
       "   (model): Qwen2Model(\n",
       "     (embed_tokens): Embedding(151936, 896)\n",
       "     (layers): ModuleList(\n",
       "       (0-23): 24 x Qwen2DecoderLayer(\n",
       "         (self_attn): Qwen2Attention(\n",
       "           (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "           (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "           (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "           (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "         )\n",
       "         (mlp): Qwen2MLP(\n",
       "           (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "           (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "           (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "         (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "       )\n",
       "     )\n",
       "     (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "     (rotary_emb): Qwen2RotaryEmbedding()\n",
       "   )\n",
       "   (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
       " ),\n",
       " Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " \t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       " }\n",
       " ))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qwen import load_qwen\n",
    "load_qwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model, tokenizer = load_qwen()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_context_target(token_ids: torch.Tensor, context_ratio: float = 0.7):\n",
    "    \"\"\"\n",
    "    Splits a 1D tensor of token IDs into context and target parts.\n",
    "    \n",
    "    Args:\n",
    "        token_ids (torch.Tensor): A 1D tensor of token IDs.\n",
    "        context_ratio (float): Fraction of tokens to use as context.\n",
    "    \n",
    "    Returns:\n",
    "        (context_ids, target_ids) (torch.Tensor, torch.Tensor)\n",
    "    \"\"\"\n",
    "    total_length = len(token_ids)\n",
    "    context_length = int(total_length * context_ratio)\n",
    "    context_ids = token_ids[:context_length]\n",
    "    target_ids = token_ids[context_length:]\n",
    "    return context_ids, target_ids\n",
    "\n",
    "def decode_tokens_to_numbers(text: str):\n",
    "    \"\"\"\n",
    "    Decodes a LLMTIME-formatted string into a list of numeric values.\n",
    "    Example format: \"0.25,1.50;0.27,1.47;0.31,1.42\"\n",
    "    \n",
    "    We split by semicolon to separate timesteps, then by comma for variables,\n",
    "    and parse each as a float.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The decoded text from the model's output.\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: A flat list of numeric values (prey, predator, prey, predator, ...).\n",
    "    \"\"\"\n",
    "    numbers = []\n",
    "    timesteps = text.split(\";\")\n",
    "    for step in timesteps:\n",
    "        # Split each timestep by commas\n",
    "        parts = step.split(\",\")\n",
    "        for p in parts:\n",
    "            try:\n",
    "                # Convert the string to float if possible\n",
    "                val = float(p.strip())\n",
    "                numbers.append(val)\n",
    "            except ValueError:\n",
    "                # If conversion fails (e.g., empty string), skip\n",
    "                continue\n",
    "    return numbers\n",
    "\n",
    "###############################################################################\n",
    "# Main Evaluation Function\n",
    "###############################################################################\n",
    "\n",
    "def evaluate_untrained_forecasting(model, tokenizer, tokenized_data, context_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Evaluate the untrained Qwen2.5-Instruct model's forecasting ability by:\n",
    "      1) Splitting each tokenized sequence into context (70%) and target (30%).\n",
    "      2) Generating predictions from the context.\n",
    "      3) Computing:\n",
    "         - Cross-entropy loss & perplexity over the entire sequence\n",
    "         - MSE of the decoded numeric predictions vs. the true target\n",
    "    \n",
    "    Args:\n",
    "        model: The untrained Qwen2.5-Instruct model from qwen.py.\n",
    "        tokenizer: The Qwen2.5-Instruct tokenizer.\n",
    "        tokenized_data (List[torch.Tensor]): List of tokenized sequences (1D tensors).\n",
    "        context_ratio (float): Fraction of tokens to use as context.\n",
    "    \n",
    "    Returns:\n",
    "        (avg_loss, avg_perplexity, avg_mse): Tuple of floats representing\n",
    "        the mean cross-entropy loss, perplexity, and mean squared error (forecast).\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    mses = []\n",
    "\n",
    "    # Evaluate on a subset (e.g. 10 sequences) for brevity\n",
    "    num_eval = min(10, len(tokenized_data))\n",
    "\n",
    "    for i in range(num_eval):\n",
    "        # 1) Retrieve the i-th tokenized sequence\n",
    "        seq = tokenized_data[i].to(device)\n",
    "\n",
    "        # 2) Split into context vs target\n",
    "        context_ids, target_ids = split_context_target(seq, context_ratio)\n",
    "\n",
    "        # 3) Generate predictions from the context\n",
    "        input_ids = context_ids.unsqueeze(0)  # add batch dimension\n",
    "        max_gen_length = len(context_ids) + len(target_ids)  # we aim to generate as many tokens as the target\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_gen_length,\n",
    "                do_sample=False  # Greedy generation\n",
    "            )\n",
    "        \n",
    "        # 4) Compute cross-entropy loss over the entire sequence (context + target)\n",
    "        #    The 'labels' argument means the model will compute language modeling loss\n",
    "        #    comparing each output token to the same shifted input token.\n",
    "        full_seq = seq.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(full_seq, labels=full_seq)\n",
    "            loss_val = output.loss.item()\n",
    "            losses.append(loss_val)\n",
    "        \n",
    "        # 5) Decode the generated tokens for the target portion\n",
    "        #    We only look at the newly generated tokens after context_ids\n",
    "        generated_ids = generated[0]\n",
    "        predicted_target_ids = generated_ids[len(context_ids):]\n",
    "\n",
    "        # Decode both predicted target and true target tokens\n",
    "        pred_text = tokenizer.decode(predicted_target_ids, skip_special_tokens=True)\n",
    "        true_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Convert them back to numeric sequences\n",
    "        pred_numbers = decode_tokens_to_numbers(pred_text)\n",
    "        true_numbers = decode_tokens_to_numbers(true_text)\n",
    "\n",
    "        # 6) Compute Mean Squared Error if the lengths match\n",
    "        if len(pred_numbers) == len(true_numbers) and len(pred_numbers) > 0:\n",
    "            mse = np.mean((np.array(pred_numbers) - np.array(true_numbers)) ** 2)\n",
    "            mses.append(mse)\n",
    "\n",
    "    avg_loss = np.mean(losses) if len(losses) > 0 else float(\"inf\")\n",
    "    avg_perplexity = float(np.exp(avg_loss)) if avg_loss != float(\"inf\") else float(\"inf\")\n",
    "    avg_mse = np.mean(mses) if len(mses) > 0 else float(\"inf\")\n",
    "\n",
    "    return avg_loss, avg_perplexity, avg_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/qwen/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Evaluation on Untrained Qwen2.5-Instruct ===\n",
      "Average Cross-Entropy Loss: 0.6168\n",
      "Average Perplexity:       1.8530\n",
      "Average MSE (Forecast):  1.0159\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_loss, avg_perplexity, avg_mse = evaluate_untrained_forecasting(\n",
    "    model, tokenizer, tokenized_test, context_ratio=0.7\n",
    ")\n",
    "\n",
    "# 4) Print results\n",
    "print(\"=== Baseline Evaluation on Untrained Qwen2.5-Instruct ===\")\n",
    "print(f\"Average Cross-Entropy Loss: {avg_loss:.4f}\")\n",
    "print(f\"Average Perplexity:       {avg_perplexity:.4f}\")\n",
    "print(f\"Average MSE (Forecast):  {avg_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Map each operation to its flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference FLOPS Calculation ===\n",
      "Number of inference steps: 1\n",
      "Batch size: 2\n",
      "Sequence length: 128\n",
      "Hidden dimension: 512\n",
      "Transformer blocks: 12\n",
      "Attention heads: 8\n",
      "FFN ratio: 4.0\n",
      "------------------------------------\n",
      "Total FLOPS for inference: 3.33e+10\n"
     ]
    }
   ],
   "source": [
    "from flops import flops_for_experiment\n",
    "\n",
    "# Suppose we want to run inference for 10 steps (e.g., 10 forward passes)\n",
    "num_steps = 1\n",
    "\n",
    "# Hypothetical model configuration\n",
    "batch_size = 2      # number of samples per batch\n",
    "seq_len = 128       # input sequence length\n",
    "hidden_dim = 512    # model hidden dimension\n",
    "num_layers = 12     # number of Transformer blocks\n",
    "num_heads = 8       # number of attention heads\n",
    "ffn_ratio = 4.0     # typical ratio for feed-forward layer size\n",
    "\n",
    "# Compute total FLOPS for the inference experiment\n",
    "total_inference_flops = flops_for_experiment(\n",
    "    num_steps=num_steps,\n",
    "    batch_size=batch_size,\n",
    "    seq_len=seq_len,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ffn_ratio=ffn_ratio,\n",
    "    training=False  # <--- Key: we are doing inference only\n",
    ")\n",
    "\n",
    "print(\"=== Inference FLOPS Calculation ===\")\n",
    "print(f\"Number of inference steps: {num_steps}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Hidden dimension: {hidden_dim}\")\n",
    "print(f\"Transformer blocks: {num_layers}\")\n",
    "print(f\"Attention heads: {num_heads}\")\n",
    "print(f\"FFN ratio: {ffn_ratio}\")\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Total FLOPS for inference: {total_inference_flops:.2e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
